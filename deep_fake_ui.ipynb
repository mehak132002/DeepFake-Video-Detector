{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Import necessary libraries"
      ],
      "metadata": {
        "id": "y_r6k1JWFv6r"
      }
    },
    {
      "source": [
        "import gradio as gr\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision import models, transforms\n",
        "import cv2\n",
        "import numpy as np\n",
        "import os\n",
        "from PIL import Image"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "FsJtHiYLFZWM"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Device Setup"
      ],
      "metadata": {
        "id": "sGUkr1iCF8vC"
      }
    },
    {
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "lysR0GGqFl3z"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Face Detection Cascade\n",
        "\n",
        " This initializes a pre-trained Haar Cascade classifier from OpenCV. This specific classifier is designed to detect frontal faces in images."
      ],
      "metadata": {
        "id": "Xvhm5FJ7GEC5"
      }
    },
    {
      "source": [
        "face_cascade = cv2.CascadeClassifier(\n",
        "    cv2.data.haarcascades + \"haarcascade_frontalface_default.xml\"\n",
        ")"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "kYljay0pFm8-"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Face Extraction Function\n",
        "\n",
        "This function, extract_faces_from_video, takes a video file path and extracts faces from a specified number of frames."
      ],
      "metadata": {
        "id": "YbSR-yIhGJzg"
      }
    },
    {
      "source": [
        "def extract_faces_from_video(\n",
        "    video_path,\n",
        "    frame_count=10,\n",
        "    output_size=(128, 128),\n",
        "    face_cascade=face_cascade\n",
        "):\n",
        "    \"\"\"\n",
        "    Extracts faces from a video at regular intervals.\n",
        "    Returns a list of cropped face images.\n",
        "    \"\"\"\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    if not cap.isOpened():\n",
        "        print(f\"Error: Could not open video file {video_path}\")\n",
        "        return []\n",
        "\n",
        "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "    if total_frames == 0:\n",
        "        print(f\"Warning: Video file {video_path} has 0 frames.\")\n",
        "        cap.release()\n",
        "        return []\n",
        "\n",
        "    step = max(total_frames // frame_count, 1)\n",
        "\n",
        "    faces = []\n",
        "    for i in range(frame_count):\n",
        "        cap.set(cv2.CAP_PROP_POS_FRAMES, i * step)\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "\n",
        "        # Convert to grayscale for face detection\n",
        "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "        # Detect faces\n",
        "        dets = face_cascade.detectMultiScale(\n",
        "            gray, scaleFactor=1.1, minNeighbors=5, minSize=(30, 30) # Added minSize for robustness\n",
        "        )\n",
        "\n",
        "        if len(dets) > 0:\n",
        "            # Get the largest face\n",
        "            x, y, w, h = max(dets, key=lambda r: r[2] * r[3])\n",
        "            face = frame[y:y+h, x:x+w]\n",
        "            face = cv2.resize(face, output_size)\n",
        "            faces.append(face)\n",
        "\n",
        "    cap.release()\n",
        "    return faces"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "fMUZRwgAFnxl"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Image Transformation\n",
        "\n",
        "This defines a sequence of image transformations that will be applied to the extracted face images before they are fed into the deep learning model."
      ],
      "metadata": {
        "id": "tbsqZy6nGYjl"
      }
    },
    {
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.Resize((128, 128)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "])"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "oyx4Ni-6Fo9V"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. Model Definition and Loading\n",
        "\n",
        "This section defines the deep learning model architecture and loads pre-trained weights."
      ],
      "metadata": {
        "id": "fWIsURB3Ge36"
      }
    },
    {
      "source": [
        "def load_model(model_path=\"/content/deepfake.pth\", device=device):\n",
        "    \"\"\"\n",
        "    Loads the ResNeXt101 model with custom final layer and trained weights.\n",
        "    \"\"\"\n",
        "    # Load pre-trained ResNeXt101\n",
        "    model = models.resnext101_32x8d(pretrained=True)\n",
        "\n",
        "    # Freeze all parameters\n",
        "    for param in model.parameters():\n",
        "        param.requires_grad = False\n",
        "\n",
        "    # Replace the final fully connected layer\n",
        "    model.fc = nn.Sequential(\n",
        "        nn.Linear(model.fc.in_features, 512),\n",
        "        nn.ReLU(),\n",
        "        nn.Dropout(0.3),\n",
        "        nn.Linear(512, 2) # 2 classes: REAL, FAKE\n",
        "    )\n",
        "\n",
        "    # Load saved state_dict\n",
        "    try:\n",
        "        # Use map_location to ensure it loads correctly regardless of original device\n",
        "        model.load_state_dict(torch.load(model_path, map_location=device))\n",
        "        print(f\"Model loaded successfully from {model_path} to {device}.\")\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: Model weights file not found at {model_path}. Please ensure it's in the same directory.\")\n",
        "        # Optionally, handle by training or downloading a default model\n",
        "        return None # Or raise an error\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading model state dict: {e}\")\n",
        "        return None\n",
        "\n",
        "    model = model.to(device)\n",
        "    model.eval() # Set model to evaluation mode\n",
        "    return model\n",
        "\n",
        "# Load the model globally to avoid reloading on each prediction\n",
        "model = load_model()"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ubFG8PAFFqHd",
        "outputId": "6d61a852-d986-4c5d-cbec-bd56166bb32f"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNeXt101_32X8D_Weights.IMAGENET1K_V1`. You can also use `weights=ResNeXt101_32X8D_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/resnext101_32x8d-8ba56ff5.pth\" to /root/.cache/torch/hub/checkpoints/resnext101_32x8d-8ba56ff5.pth\n",
            "100%|██████████| 340M/340M [00:02<00:00, 133MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model loaded successfully from /content/deepfake.pth to cpu.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6. Prediction Function for Gradio\n",
        "\n",
        "This function, predict_deepfake, takes the video file path as input and returns a string indicating whether the video is classified as REAL or FAKE and the confidence level."
      ],
      "metadata": {
        "id": "t7v2G4INGiW2"
      }
    },
    {
      "source": [
        "def predict_deepfake(video_path):\n",
        "    \"\"\"\n",
        "    Predicts if a video contains a deepfake.\n",
        "    Returns prediction text for UI, and raw data for state.\n",
        "    \"\"\"\n",
        "    if model is None:\n",
        "        return \"Error: Model not loaded. Cannot perform prediction.\", None, None, None\n",
        "\n",
        "    try:\n",
        "        faces = extract_faces_from_video(video_path)\n",
        "\n",
        "        if not faces:\n",
        "            print(\"Debug: No faces detected in the video.\")\n",
        "            return \"No faces detected in the video. Cannot classify.\", None, None, None\n",
        "\n",
        "        face_tensors = [transform(Image.fromarray(cv2.cvtColor(face, cv2.COLOR_BGR2RGB))) for face in faces]\n",
        "\n",
        "        if not face_tensors:\n",
        "            print(\"Debug: Error during face tensor creation.\")\n",
        "            return \"Error during face processing.\", None, None, None\n",
        "\n",
        "        inputs = torch.stack(face_tensors).to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = model(inputs)\n",
        "            probabilities = torch.softmax(outputs, dim=1)\n",
        "            avg_probabilities = torch.mean(probabilities, dim=0)\n",
        "            predicted_class_idx = torch.argmax(avg_probabilities).item()\n",
        "            confidence = avg_probabilities[predicted_class_idx].item()\n",
        "\n",
        "        labels = [\"REAL\", \"FAKE\"]\n",
        "        predicted_label_str = labels[predicted_class_idx]\n",
        "\n",
        "        result_text = f\"Prediction: {predicted_label_str}\\nConfidence: {confidence:.2f}\"\n",
        "\n",
        "        # Return prediction details for the state component\n",
        "        return result_text, video_path, predicted_label_str, confidence\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"An unexpected error occurred during prediction: {e}\")\n",
        "        return f\"Prediction Error: An unexpected error occurred. Details: {e}\", None, None, None"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "XmElYqEyFrBt"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 7. CSV File Setup\n",
        " It defines the filename and headers for a CSV file that stores flagged videos information"
      ],
      "metadata": {
        "id": "Gj4puRDourKe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- CSV File Setup ---\n",
        "FLAGGED_VIDEOS_CSV = \"flagged_videos.csv\"\n",
        "CSV_HEADERS = [\"timestamp\", \"video_path\", \"model_prediction\", \"model_confidence\", \"user_flag\"]\n",
        "\n",
        "# Initialize CSV file with headers if it doesn't exist\n",
        "if not os.path.exists(FLAGGED_VIDEOS_CSV):\n",
        "    df = pd.DataFrame(columns=CSV_HEADERS)\n",
        "    df.to_csv(FLAGGED_VIDEOS_CSV, index=False)"
      ],
      "metadata": {
        "id": "Ba0mjcqzVnSe"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 8. Flag Function\n",
        "The function takes the video path, model prediction, model confidence, and user's true label as input. It records this information along with a timestamp and appends it to the csv file we defined earlier."
      ],
      "metadata": {
        "id": "uZu06y4rvEKA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Flagging Function ---\n",
        "def flag_video(\n",
        "    video_path,\n",
        "    model_prediction,\n",
        "    model_confidence,\n",
        "    user_true_label\n",
        "):\n",
        "    \"\"\"\n",
        "    Flags the video information to a CSV file if there's a mismatch or it's flagged by user.\n",
        "    \"\"\"\n",
        "    if not video_path: # No video uploaded yet\n",
        "        return \"Please upload and get a prediction first.\"\n",
        "    if user_true_label == \"Not specified\": # User didn't select\n",
        "        return \"Please select 'This is REAL' or 'This is FAKE' to flag.\"\n",
        "\n",
        "    timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "\n",
        "    # Create a DataFrame for the new entry\n",
        "    new_entry = pd.DataFrame([{\n",
        "        \"timestamp\": timestamp,\n",
        "        \"video_path\": video_path,\n",
        "        \"model_prediction\": model_prediction,\n",
        "        \"model_confidence\": f\"{model_confidence:.2f}\", # Format confidence\n",
        "        \"user_flag\": user_true_label\n",
        "    }])\n",
        "\n",
        "    # Append to CSV\n",
        "    try:\n",
        "        if not os.path.exists(FLAGGED_VIDEOS_CSV):\n",
        "            new_entry.to_csv(FLAGGED_VIDEOS_CSV, index=False)\n",
        "        else:\n",
        "            new_entry.to_csv(FLAGGED_VIDEOS_CSV, mode='a', header=False, index=False)\n",
        "\n",
        "        # Check if it's a mismatch for the feedback message\n",
        "        if model_prediction != user_true_label:\n",
        "            return f\"Video flagged as a MISMATCH!\\nDetails saved to {FLAGGED_VIDEOS_CSV}\"\n",
        "        else:\n",
        "            return f\"Video flagged (model prediction matches user's view).\\nDetails saved to {FLAGGED_VIDEOS_CSV}\"\n",
        "\n",
        "    except Exception as e: # Catch any exception during CSV operations\n",
        "        print(f\"Error during flagging to CSV: {e}\") # Print to console for debugging\n",
        "        return f\"Error saving flag to CSV: {e}\""
      ],
      "metadata": {
        "id": "j97KCPeqQjeM"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 9. Gradio Interface"
      ],
      "metadata": {
        "id": "KDxgAa0uvbME"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 7. Gradio Interface ---\n",
        "with gr.Blocks() as iface:\n",
        "  #markdown for title and description\n",
        "    gr.Markdown(\n",
        "        \"\"\"\n",
        "        # Deepfake Video Detector\n",
        "        Upload a video to detect if it contains a deepfake using a ResNeXt101 model.\n",
        "        You can also flag videos if you believe the model's prediction is incorrect.\n",
        "        Ensure 'resnext101_deepfake_faces.pth' and 'haarcascade_frontalface_default.xml' are available.\n",
        "        \"\"\"\n",
        "    )\n",
        "\n",
        "    with gr.Row():\n",
        "        video_input = gr.Video(label=\"Upload Video\")\n",
        "        result_output = gr.Textbox(label=\"Deepfake Detection Result\")\n",
        "\n",
        "    # Hidden state components to pass information between prediction and flagging\n",
        "    video_path_state = gr.State(value=None)\n",
        "    model_prediction_state = gr.State(value=None)\n",
        "    model_confidence_state = gr.State(value=None)\n",
        "\n",
        "    # Clear states and result when a new video input is initiated\n",
        "    video_input.change(\n",
        "        fn=lambda x: [None, None, None, None, \"Ready for prediction...\"],\n",
        "        inputs=video_input, # Use video_input as input to capture change event\n",
        "        outputs=[video_path_state, model_prediction_state, model_confidence_state, result_output],\n",
        "        queue=False # This listener should run quickly\n",
        "    )\n",
        "\n",
        "    # When video input changes, trigger prediction and update state variables\n",
        "    video_input.upload(\n",
        "        fn=predict_deepfake,\n",
        "        inputs=video_input,\n",
        "        outputs=[result_output, video_path_state, model_prediction_state, model_confidence_state],\n",
        "        show_progress=True\n",
        "    )\n",
        "    # Also handle the case where video is uploaded via drag-and-drop or Browse (not just changing content)\n",
        "    # This 'change' listener will trigger predict_deepfake if the video content itself changes.\n",
        "    # It might be redundant with 'upload' if 'upload' covers all user interactions for input.\n",
        "    # We will keep it for robustness, but ensure outputs are cleared on *any* change.\n",
        "    video_input.change(\n",
        "        fn=predict_deepfake,\n",
        "        inputs=video_input,\n",
        "        outputs=[result_output, video_path_state, model_prediction_state, model_confidence_state],\n",
        "        show_progress=True\n",
        "    )\n",
        "\n",
        "\n",
        "    gr.Markdown(\n",
        "        \"\"\"\n",
        "        ### Flag Inaccurate Predictions\n",
        "        If you believe the model's prediction is incorrect, you can flag the video here.\n",
        "        This helps in potential future model improvements.\n",
        "        \"\"\"\n",
        "    )\n",
        "    with gr.Row():\n",
        "        user_label_radio = gr.Radio(\n",
        "            [\"This is REAL\", \"This is FAKE\"],\n",
        "            label=\"What is the TRUE label of this video?\",\n",
        "            value=\"Not specified\" # Default state\n",
        "        )\n",
        "        flag_button = gr.Button(\"Flag Video\")\n",
        "        flag_status_output = gr.Textbox(label=\"Flag Status\")\n",
        "\n",
        "    # Flag button click event\n",
        "    flag_button.click(\n",
        "        fn=flag_video,\n",
        "        inputs=[video_path_state, model_prediction_state, model_confidence_state, user_label_radio],\n",
        "        outputs=flag_status_output\n",
        "    )\n",
        "\n",
        "    # Clear user radio selection after flagging (optional, but good UX)\n",
        "    flag_button.click(\n",
        "        fn=lambda: \"Not specified\",\n",
        "        inputs=None,\n",
        "        outputs=user_label_radio,\n",
        "        queue=False # Do not wait for this to finish\n",
        "    )\n",
        "\n",
        "\n",
        "# Launch the interface\n",
        "if __name__ == \"__main__\":\n",
        "    print(f\"Running on device: {device}\")\n",
        "    iface.launch()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 663
        },
        "id": "7Woo2Hs5Q1OQ",
        "outputId": "d7b79256-69a9-448f-e8fe-49e1bfaae3fa"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running on device: cpu\n",
            "It looks like you are running Gradio on a hosted a Jupyter notebook. For the Gradio app to work, sharing must be enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://a3858598d4366c9e1e.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://a3858598d4366c9e1e.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}